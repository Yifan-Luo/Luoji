<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/carrot_32px.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/carrot_16px.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <link rel="alternate" href="/atom.xml" title="Luoji" type="application/atom+xml">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|[object Object]:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.5.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":true,"scrollpercent":true},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="Daily notes">
<meta name="keywords" content="diary">
<meta property="og:type" content="article">
<meta property="og:title" content="Diary">
<meta property="og:url" content="http:&#x2F;&#x2F;luoji-neuron.tk&#x2F;2019&#x2F;11&#x2F;11&#x2F;notes%20diary&#x2F;index.html">
<meta property="og:site_name" content="Luoji">
<meta property="og:description" content="Daily notes">
<meta property="og:locale" content="en">
<meta property="og:image" content="https:&#x2F;&#x2F;i.imgur.com&#x2F;vDAobu1.png">
<meta property="og:image" content="http:&#x2F;&#x2F;luoji-neuron.tk&#x2F;2019&#x2F;11&#x2F;11&#x2F;notes%20diary&#x2F;RL_mn_01.png">
<meta property="og:updated_time" content="2021-08-17T14:26:17.631Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;i.imgur.com&#x2F;vDAobu1.png">

<link rel="canonical" href="http://luoji-neuron.tk/2019/11/11/notes%20diary/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Diary | Luoji</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Luoji</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="Searching..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="en">
    <link itemprop="mainEntityOfPage" href="http://luoji-neuron.tk/2019/11/11/notes%20diary/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/smile.jpg">
      <meta itemprop="name" content="Yifan Luo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Luoji">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Diary
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-11-11 19:56:14" itemprop="dateCreated datePublished" datetime="2019-11-11T19:56:14+01:00">2019-11-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-08-17 16:26:17" itemprop="dateModified" datetime="2021-08-17T16:26:17+02:00">2021-08-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/diary/" itemprop="url" rel="index">
                    <span itemprop="name">diary</span>
                  </a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2019/11/11/notes%20diary/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/11/11/notes diary/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>Daily notes</p>
<a id="more"></a>
<h1 id="2021"><a href="#2021" class="headerlink" title="2021"></a>2021</h1><h2 id="week-July-3"><a href="#week-July-3" class="headerlink" title="week July 3"></a>week July 3</h2><h3 id="22-07"><a href="#22-07" class="headerlink" title="22/07"></a>22/07</h3><ul>
<li><p>Machine learning is the science of getting computers to act <u>without being explicitly programmed</u>.<br><a href="https://www.coursera.org/learn/machine-learning#syllabus" target="_blank" rel="noopener">Machine Learning, coursera, Offered by Stanford University</a></p>
<blockquote>
<p>Machine learning is pre-processing the information to the higher level useage.<br>This course provides a broad introduction to machine learning, datamining, and statistical pattern recognition. Topics include: (i) <strong>Supervised learning</strong> (parametric/non-parametric algorithms, support vector machines, kernels, neural networks). (ii) <strong>Unsupervised learning</strong> (<strong><em>clustering</em></strong>, <strong><em>dimensionality reduction</em></strong>, recommender systems, deep learning). (iii) Best practices in machine learning (bias/variance theory; innovation process in machine learning and AI). The course will also draw from numerous case studies and applications, so that you’ll also learn how to apply learning algorithms to building smart robots (perception, control), text understanding (web search, anti-spam), computer vision, medical informatics, audio, database mining, and other areas.</p>
</blockquote>
</li>
<li><p>机器学习是人工智能的一个分支。人工智能的研究历史有着一条从<em>以“推理”为重点</em>，到<em>以“知识”为重点</em>，再到<em>以“学习”为重点</em>的自然、清晰的脉络。</p>
</li>
<li><p>机器学习理论主要是设计和分析一些让<em>计算机可以自动“学习”</em>算法。</p>
<blockquote>
<p>design the information flow, to form the structure of information.（什么层级应该存什么memory，不同层级间的memory如何互动）</p>
</blockquote>
</li>
<li><p>机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。因为学习算法中涉及了大量的统计学理论，机器学习与推断统计学联系尤为密切，也被称为<strong>统计学习理论</strong>。</p>
<blockquote>
<p>从数据中榨取信息。</p>
</blockquote>
</li>
<li><p>机器学习有下面几种定义：</p>
<ul>
<li>机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能。</li>
<li>机器学习是对能通过经验自动改进的计算机算法的研究。</li>
<li>机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。</li>
<li>一种经常引用的英文定义是：A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.<blockquote>
<p>机器学习是将information从底层level提到高level。（提level的过程中有很多不可避免的信息损失）</p>
</blockquote>
</li>
</ul>
</li>
<li><p>以information为考量时，不同level的信息会受到由于本层信息的特性而受到的干扰。</p>
</li>
<li><p>Taking brain as the goal or a example for AI is <strong>very very dangerous</strong>.</p>
</li>
<li><p>Thinking, Fast and Slow</p>
<ul>
<li>system 1 and system 2</li>
<li>causality. If two pieces of information have causality relationship, this means that an conditioned reflex can be put between them. This knowledge can be put to system 1.</li>
</ul>
</li>
<li><p>when you are doing language studies, better have in mind that the real information which the sentence is trying to convey (the real information is in the brain).</p>
</li>
</ul>
<h3 id="23-07"><a href="#23-07" class="headerlink" title="23/07"></a>23/07</h3><ul>
<li><p>李国杰院士 – 李国杰. 大数据研究的科学价值 [J]. Diss. 2012. <a href="http://www.ict.ac.cn/liguojiewenxuan_162523/wzlj/lgjxsbg/201912/P020191227654597760261.pdf" target="_blank" rel="noopener">(论文pdf链接)</a></p>
<blockquote>
<p>数据背后的共性 - 关系网络。<br>大数据面临的科学问题本质上可能就是网络科学问题。复杂网络分析应该是数据科学的重要基石。<br>与传统的逻辑推理研究不同，大数据研究是对数量巨大的数据做统计性的搜索、比较、聚类和分类等分析归纳，因此继承了统计科学的一些特点。</p>
</blockquote>
</li>
<li><p>网络科学就是对information间相互关系的简单性化刻画，在节点之间进行信息的传播，聚合，以及变换。</p>
</li>
<li><p>拓扑结构是信息间相互关系。</p>
</li>
<li><p>intelligence is a problem of how to effective use the information at hand.</p>
</li>
<li><p>consciousness is the ability to explain the incoming information.</p>
</li>
<li><p>The representation of position in the mammalian brain is distributed across multiple neural populations.</p>
</li>
<li><p>Attention是information compression的补偿措施。</p>
</li>
<li><p>recurrent independent mechanism. instead of a homogeneous neural net, we break the architecture into small modules, causal mechanis, instead of  everything connect to everything, it’s fully connected inside each module, but <u>between modules, there is a strong bottleneck</u>,  very few bits of information are allowed to be communicated between the modules. modules also compete, only the modules which have interesting thing to say are able to communicate to others.</p>
</li>
</ul>
<h3 id="25-07"><a href="#25-07" class="headerlink" title="25/07"></a>25/07</h3><ul>
<li><p>The complexity of the data creates a need for statistical summary.</p>
</li>
<li><p>Kriegeskorte, Nikolaus, Rainer Goebel, and Peter Bandettini. 2006. “Information-Based Functional Brain Mapping.” Proceedings of the National Academy of Sciences 103 (10): 3863–68. <a href="https://doi.org/10.1073/pnas.0600244103" target="_blank" rel="noopener">link</a>.</p>
<ul>
<li>Functional brain mapping has evolved from the idea that the brain consists of functionally specialized macroscopic regions.</li>
<li>We propose <strong><em>information-based functional brain mapping</em></strong>, <u>an alternative to the classical activation-based approach</u>. The <em>core idea</em> is to localize functional regions carrying a particular type of information by scanning the entire volume with a multivariate searchlight.</li>
<li>It is natural to use information-theoretic measures to quantify activity-pattern information.</li>
</ul>
</li>
<li><p><strong>空间是有结构的集合</strong>.<u>什么叫结构呢？系统论里面有这个概念，就是两个元素之间有关系，而所有元素之间关系的整体就构成了结构</u>.</p>
<blockquote>
<p>什么是空间。首先空间是一个集合，也就是集合的性质，空间都有。那么空间和集合的区别在于<u>集合没有结构，而空间有结构</u>，这样一来，我们看的很清楚：<strong>空间是有结构的集合</strong>。现在第二个问题来了，<u>什么叫结构呢？系统论里面有这个概念，就是两个元素之间有关系，而所有元素之间关系的整体就构成了结构</u>。我们举一个例子，实数集就是一个集合，这个集合由元素组成，元素之间没有任何关系，我们只关注这个集合元素的唯一性和无序性等特点。现在我们考查实数直线集，这就是空间了，因为实数直线的元素也就是点之间有大小的关系，也就是每一个元素都能比较大小，能够排列起来，这也就是一维空间了。认识一个集合不如认识一个空间更了解一个事物，而了解一个空间就是了解这个空间元素之间的关系，也就是结构，一旦彻底认识了空间的结构，很多问题就能简化了，比如线性空间，一旦确定了一组基，那么所有的元素就都容易表达了，也就是使用这组基的线性组合表达出来即可。这样一个公式完全表示了所有元素，这就是结构的妙用。</p>
</blockquote>
</li>
<li><p><u>引进空间的目的是方便讨论这个空间上函数的积分性质</u>，也就是为了讨论关于这个空间的一个集合上的函数的性质，以及函数列性质，为以后讨论这个函数的积分做准备，因为可测这个概念的目的就是为了函数积分提出来的，如果不可测，那么积分也就不存在。</p>
</li>
<li><p>抽象代数.</p>
<blockquote>
<p>抽象代数作为数学的一门学科，主要研究对象是<strong>代数结构</strong>，比如群、环、域、模、向量空间、格与域代数。“抽象代数”一词出现于20世纪初，作为与其他代数领域相区别之学科。</p>
</blockquote>
</li>
<li><p>实变、泛函、抽代、拓扑 (四大数学系本科最核心课程)，基本上学完上述课程才算迈入了近代数学的大门。</p>
<blockquote>
<p>实变里面的测度论是概率论的基础，控制收敛定理等在概率论理论推导里也很有用。泛函在控制理论里面相当重要且基础。抽代可应用于密码学与编码理论。拓扑可应用的领域包括传感器网络、机器人设计、以及材料里面的大分子结构等等。另外拓扑在数据分析里面也很有用，现在是大数据时代，<u>我的理解是可以把离散的数据点看成某个流形的离散化，从而可以用拓扑的方法研究数据的结构</u>。所以这四门学科在我看来都是有应用背景的，但是要学到一定程度才能用好。<u>本科一学期的同名数学专业课程是显然不够的</u>。<br>数学本身分类就分为纯数，应数，计算三个大类。泛函绝对是应用最广的，因为它能搞定微分方程(任何靠谱的理科都需要微分方程)。 but 偏微分有很多自身的技巧，估计方法。但是一般来说，做偏微分，你必须要懂泛函。懂得越多越好。<u>很难想象一个不懂实变，不懂拓扑的可以学好泛函</u>。</p>
</blockquote>
</li>
</ul>
<h2 id="week-July-4"><a href="#week-July-4" class="headerlink" title="week July 4"></a>week July 4</h2><h3 id="26-07"><a href="#26-07" class="headerlink" title="26/07"></a>26/07</h3><ul>
<li><p>从伽利略到牛顿，再到达尔文，再到现在的科学家，他们的研究一次又一次地削弱了教条主义的禁锢。</p>
<blockquote>
<p>教条主义只是在寻找合适的 constraints 限制manifold上的information flow.教条主义无罪，只是迫于当时条件，只能给出boundary很不适合的constrants.</p>
</blockquote>
</li>
<li><p>Dyna Architecture</p>
<blockquote>
<p><img src="https://i.imgur.com/vDAobu1.png" alt="Dyna算法框架"><br><img src="/2019/11/11/notes%20diary/RL_mn_01.png" alt="RL Dyna_me"></p>
</blockquote>
</li>
<li><p>在模仿学习中我们试图从获取的大量的数据（输入观测o，输出动作a）中进行学习，最终得到一个策略网络：这样的方法由于<u>实际数据和训练数据分布不一致往往很难取得效果</u>。然后我们尝试使用<strong>动态规划（值迭代、策略迭代）</strong>、<strong>蒙特卡洛(MC)</strong>以及<strong>时间差分(TD)</strong>三种方法来解决马尔可夫决策过程描述的强化学习任务。其中动态规划虽然只bootstrapping一次但需要知道环境转移模型，而蒙特卡洛虽然无需知道环境转移模型但每次都要一个完整的episode，因此结合了两者的时间差分就显得非常有效。我们在DQN中进一步将时间差分中的Q-learning和深度学习结合构建价值网络，训练了一个非常有趣的能打pong游戏的agent。随后我们从直接最大化回报函数的角度，学习了策略梯度方法，它将策略网络和价值网络相结合，发展出了著名的Actor Critic算法。</p>
</li>
<li><p>基于价值的强化学习模型和基于策略的强化学习模型都不是基于模型的，它们从价值函数，策略函数中直接去学习，不用学习环境的状态转化概率模型，即在状态s下采取动作a,转到下一个状态s′的概率$P^a_{ss′}$。而基于模型的强化学习则会尝试从环境的模型去学习，一般是下面两个相互独立的模型：一个是<strong>状态转化预测模型</strong>，输入当前状态s和动作a，预测下一个状态s′。另一个是<strong>奖励预测模型</strong>，输入当前状态s和动作a，预测环境的奖励r。从上面的描述我们可以看出基于模型的强化学习和不基于模型的强化学习的主要区别：即<strong>基于模型的强化学习是从模型中学习</strong>，而<strong>不基于模型的强化学习是从和环境交互的经历去学习</strong>。<strong>基于模型的强化学习一般不单独使用</strong>，而是和不基于模型的强化学习结合起来，因此使用Dyna算法框架是常用的做法。</p>
</li>
<li><p>Learning is a pattern formation process of information.</p>
<ul>
<li>hierarchies of the information</li>
<li>structures of the information flow.</li>
</ul>
</li>
<li><p>生命是什么？生命是在各种constrants下的一种特殊的information process machine.</p>
</li>
<li><p>宇宙胚胎种源论，是一种假说，猜想各种形态的微生物存在于全宇宙，并借着流星、小行星与彗星散播、繁衍。</p>
</li>
<li><p>cognitive map. The term “cognitive map” originated with Tolman. And Tolman, who was working at a time when American psychology was dominated by behaviorism, he really struck out in a different direction. He did some very clever experiments that seemed to <strong>argue against the prevailing behaviorist dogma that all learning was based on reinforcement</strong>.</p>
</li>
</ul>
<h3 id="27-07"><a href="#27-07" class="headerlink" title="27/07"></a>27/07</h3><ul>
<li><p>The model-based approach is computationally inefficient because it requires planning, but it’s very flexible.</p>
</li>
<li><p>人工智能是用计算机来模拟<del>大脑的</del>信息处理即思维决策过程。</p>
<blockquote>
<p>人工智能是模拟信息处理过程，而不是大脑的，大脑只是一个例子。</p>
</blockquote>
</li>
<li><p>流场模态分解（<strong>modal decomposition</strong>）是近年来一个比较有意思的流体研究方向，事实上各种模态分解方法的数学原理在非常早的时期就已建立完成，但受限于彼时CFD、实验技术的限制，这方面的探究主要局限于数学推导上。随着计算流体力学的兴起、数据/人工智能（机器学习）时代的浪潮驱使，它再一次成为大家关注的一个兴趣点（ARFM：17年一篇Modal reduction ；20年一篇Machine learning）。</p>
<blockquote>
<p>流场模态分解大体分为两类：</p>
</blockquote>
<ul>
<li>data-driven 数据驱动 （POD、SPOD、DMD）</li>
<li>operator-driven 算子驱动 （Koopman analysis、Global linear、stability analysis、Resolvent analysis）</li>
</ul>
</li>
</ul>
<h3 id="28-07"><a href="#28-07" class="headerlink" title="28/07"></a>28/07</h3><ul>
<li><p>Continual Learning and Catastrophic Forgetting</p>
<ul>
<li>Continual Learning, learning from the inputs to update the internal representation</li>
<li>Catastrophic Forgetting, think about this when the information you needed has been forgotten.</li>
</ul>
</li>
<li><p>图神经网络凭借其在处理<strong>非欧氏空间数据</strong>的优越性能，在社交网络、推荐系统、药物发现任务和交通预测等领域大放异彩。</p>
</li>
<li><p>Notably, grid cell activity is more local than previously thought and as a consequence cannot provide a universal spatial metric in all environments.</p>
</li>
</ul>
<h3 id="29-07"><a href="#29-07" class="headerlink" title="29/07"></a>29/07</h3><ul>
<li><p>博弈论是智能体间的信息在一定限制条件下的竞相表达。</p>
</li>
<li><p>博弈论是经济学和应用数学的一个分支，是专门研究有合作和竞争的多人博弈中的决策者的一些行为和策略。</p>
</li>
<li><p>Many real technological, biological, social, and information networks fall into the broad class of ‘small-world’ networks, <em>a middle ground between regular and random networks</em>: they have <strong>high local clustering of elements, like regular networks</strong>, but also <strong>short path lengths between elements, like random networks</strong>.</p>
</li>
<li><p><strong>One popular method for studying small-world networks</strong> is to use an equivalent network model to generate other similar instances of the class of systems under study.</p>
</li>
<li><p>Diffusion dynamics in small-world networks.</p>
</li>
<li><p>A key concept in defining small-worlds networks is that of ‘clustering’ which measures the extent to which the neighbors of a node are also interconnected.</p>
</li>
<li><p>The objective of conventional brain mapping is to identify which brain regions are reliably more active as a function of different kinds of stimulation or manipulations of mental state (in addition to error, E). The classical outcome of brain mapping study is a parametric map indicating the extent to which every brain measure (voxel) is associated with a given mental state. The objective of developing a multivariate brain model is to account for, and thus predict, an individual person’s mental state or behavior (outcomes) based on their brain activity.</p>
</li>
<li><p>Bokeria, Levan, Richard N. Henson, and Robert M. Mok. 2021. “Map-Like Representations of an Abstract Conceptual Space in the Human Brain.” Frontiers in Human Neuroscience 0. <a href="https://doi.org/10.3389/fnhum.2021.620056" target="_blank" rel="noopener">link</a>.</p>
<ul>
<li><p>Behrens, Timothy E. J., Timothy H. Muller, James C. R. Whittington, Shirley Mark, Alon B. Baram, Kimberly L. Stachenfeld, and Zeb Kurth-Nelson. 2018. “<strong>What Is a Cognitive Map? Organizing Knowledge for Flexible Behavior</strong>.” Neuron 100 (2): 490–509. <a href="https://doi.org/10.1016/j.neuron.2018.10.002" target="_blank" rel="noopener">link</a>.</p>
<blockquote>
<p>It is proposed that <strong>a cognitive map encoding the <u>relationships</u> between entities</strong> in the world supports flexible behavior<br>These principles describe how to learn and use abstract, generalizable knowledge and suggest that map-like representations observed in a spatial context may be an instance of general coding mechanisms capable of organizing knowledge of all kinds.</p>
</blockquote>
</li>
<li><p>Bellmund, Jacob L. S., Peter Gärdenfors, Edvard I. Moser, and Christian F. Doeller. 2018. “<strong>Navigating Cognition: Spatial Codes for Human Thinking</strong>.” Science 362 (6415): eaat6766. <a href="https://doi.org/10.1126/science.aat6766" target="_blank" rel="noopener">link</a>.</p>
</li>
<li><p>Theves, S., Fernández, G., and Doeller, C. F. (2020). <u><strong>The hippocampus maps concept space, not feature space</strong></u>. J. Neurosci. 40, 7318–7325. doi: 10.1523/JNEUROSCI.0494-20.2020</p>
</li>
<li><p>Theves, Stephanie, Guillén Fernandez, and Christian F. Doeller. 2019. “The Hippocampus Encodes Distances in Multidimensional Feature Space.” Current Biology 0 (0). <a href="https://doi.org/10.1016/j.cub.2019.02.035" target="_blank" rel="noopener">link</a>.</p>
</li>
</ul>
</li>
</ul>
<ul>
<li>Viganò, S., and Piazza,M. (2020).Distance and direction codes underlie navigation of a novel semantic space in the human brain. J. Neurosci. 40, 2727–2736. doi: 10.1523/JNEUROSCI.1849-19.2020<blockquote>
<p>A recent study in the Journal of Neuroscience (Viganò and Piazza, 2020) takes this research further by suggesting that the entorhinal cortex (EHC) and the mPFC are capable of mapping not only sensory spaces, but also abstract semantic spaces.</p>
</blockquote>
</li>
</ul>
<ul>
<li><p>Humphries, Mark D., and Kevin Gurney. 2008. “Network ‘Small-World-Ness’: A Quantitative Method for Determining Canonical Network Equivalence.” PLOS ONE 3 (4): e0002051. <a href="https://doi.org/10.1371/journal.pone.0002051" target="_blank" rel="noopener">link</a>.</p>
<ul>
<li><p>Newman MEJ (2003) The structure and function of complex networks. SIAM Review 45: 167–256. (Cited by 21587)</p>
</li>
<li><p>Boccaletti S, Latora V, Moreno Y, Chavez M, Hwang DU (2006) Complex networks: Structure and function. Phys Rep 424: 175–308. (Cited by 10924)</p>
</li>
<li><p>Delre S, Jager W, Janssen M (2007) Diffusion dynamics in small-world networks with heterogeneous consumers. Comput Math Organ Theory 13: 185–202.</p>
</li>
</ul>
</li>
</ul>
<h3 id="30-07"><a href="#30-07" class="headerlink" title="30/07"></a>30/07</h3><ul>
<li><p>Animals appear to possess an internal representation of space or ‘cognitive map’. Three neurons are thought to form the basis of this map: place, head direction and grid cells. There are many other, less well understood types of spatially sensitive neuron.</p>
</li>
<li><p>Flexible behaviors of (find shortcuts) inspired Tolman to coin the term “cognitive map,” referring to a <strong>rich internal model of the world</strong> that accounts for the relationships between events and predicts the consequences of actions. For Tolman, this cognitive map was a <strong>systematic organization of knowledge that spanned all domains of behavior</strong></p>
</li>
<li><p>grid cells encode <strong>statistical regularities</strong> in spatial navigation that occur due to the common structure of all two-dimensional spaces.</p>
</li>
<li><p>历史课的作用：用真实的事件、逻辑，把更基于事实的信息流动方式写入大脑。</p>
</li>
</ul>
<h3 id="31-07"><a href="#31-07" class="headerlink" title="31/07"></a>31/07</h3><ul>
<li><p>parasubiculum</p>
<ul>
<li>The parasubiculum is a major component of the hippocampal formation that receives inputs from the CA1 region, anterior thalamus, and medial septum and that projects primarily to layer II of the entorhinal cortex.</li>
<li>It is postulated that this area may play an integral role in spatial navigation and the integration of head-directional information (Chrobak &amp; Buzsáki, 1994; Taube, 1995)</li>
</ul>
</li>
<li><p>We report the spatial and temporal properties of a class of cells termed <strong>theta-modulated place-by-direction (TPD) cells</strong> recorded from the <strong>presubicular and parasubicular</strong> cortices of the rat. <a href="https://www.jneurosci.org/content/24/38/8265" target="_blank" rel="noopener">(Cacucci et al. 2004)</a></p>
</li>
<li><p>Reynolds, John H., and David J. Heeger. 2009. “The Normalization Model of Attention.” Neuron 61 (2): 168–85. <a href="https://doi.org/10.1016/j.neuron.2009.01.002" target="_blank" rel="noopener">link</a>. (Cited by 1228)</p>
<ul>
<li><strong>unifying a range of experimental data within a common computational framework</strong>.</li>
<li>Attention has been found to have a wide variety of effects on the responses of neurons in visual cortex.</li>
<li>Attention has been known to play a central role in perception since the dawn of experimental psychology (James, 1890). Neuroscientists have utilized a variety of techniques (single-unit electrophysiology, electrical microstimulation, functional imaging, and visual-evoked potentials) to map the network of brain areas that <strong>mediate the allocation of attention</strong> (Corbetta and Shulman, 2002; Yantis and Serences, 2003) and <strong>to examine how attention modulates neuronal activity</strong> in visual cortex.</li>
</ul>
</li>
</ul>
<ul>
<li>冯伟民：黑暗生物群与生命起源<a href="https://www.youtube.com/watch?v=kvKlNdrVgxw" target="_blank" rel="noopener">一席 video link</a></li>
</ul>
<h2 id="week-August-1"><a href="#week-August-1" class="headerlink" title="week August 1"></a>week August 1</h2><h3 id="01-08"><a href="#01-08" class="headerlink" title="01/08"></a>01/08</h3><ul>
<li><p>门外的人们，会把科学界暂时达成的一个妥协当做是真理，比科学家还要狂热的捍卫。</p>
</li>
<li><p>智慧就是自知无知。 – 苏格拉底</p>
</li>
<li><p>安阳中国文字博物馆。从陶器上的刻画符号，发展到甲骨文。甲骨文是一种六书具备的问题。“六书”是古人解说汉字的结构和使用方法而归纳出来的六种条例。“六书”之名，最早见于《周礼·地官·保氏》。后世学者定名为象形、指事、会意、形声、转注、假借。</p>
<ul>
<li>一曰指事。指事者，视而可识，察而见意，上下是也；</li>
<li>二曰象形。象形者，画成其物，随体诘诎，日月是也；</li>
<li>三曰形声。形声者，以事为名，取譬相成，江河是也；</li>
<li>四曰会意。会意者，比类合谊，以见指㧑，武信是也；</li>
<li>五曰转注。转注者，建类一首，同意相受，考老是也；</li>
<li>六曰假借。假借者，本无其字，依声托事，令长是也。</li>
</ul>
</li>
<li><p>Murphy, Andrew C., Maxwell A. Bertolero, Lia Papadopoulos, David M. Lydon-Staley, and Danielle S. Bassett. 2020. “Multimodal Network Dynamics Underpinning Working Memory.” Nature Communications 11 (1): 3035. <a href="https://doi.org/10.1038/s41467-020-15541-0" target="_blank" rel="noopener">link</a>.</p>
<ul>
<li>尽管很多认知过程可以被映射到单个脑区的功能上，但是复杂的认知活动需要依靠各脑区之间的协同工作。工作记忆（working memory; WM）就是一个很好的例子，它一方面受到额顶网络（FPN）的支持，同时也受到默认网络（DMN）的影响。</li>
<li>前人研究发现，这两个网络在高WM负荷情况下的功能是竞争性的，也就是两个系统的活动存在反向相关。而且，这种反向相关的模式可以预测WM的行为表现。</li>
<li>作者认为，FPN的动态会直接调节FPN和DMN之间的连接强度，他们提出的假设机制是：FPN由两个独立的、不重叠的子网络组成。其中，子网络A与DMN的动态正向相关，子网络B和DMN的动态反向相关（图1a)。当第一个子网络的相对活动比较强的时候，DMN和FPN之间的连接就是正向的；第二个子网络相对活动强的时候，DMN和FPN之间的连接就是负向的。这个假说可以解释DMN和FPN之间功能连接的灵活动态变化，也可以解释WM个体差异与网络间连接强度的关系。</li>
<li>总结</li>
<li>综上所述，本研究先利用无监督的聚类分析将FPN区分成了两个不重叠的子网络，其一在功能上与DMN一致，其二与DAN功能上一致。这两个子系统的相对活动水平可以使FPN的功能与DMN或DAN的功能趋同。无论是结构连接还是功能连接，子网络A都与DMN的连接更紧密，子网络B与DAN连接更紧密。最后，无论是脑影像数据还是非线性动力系统建模，都表明两个子网络的强度可以不同方向影响FPN与DMN、FPN与DAN之间的功能连接强度。总之，在工作记忆任务中，FPN（而非DMN）可以部分控制着FPN与DMN之间的连接。</li>
<li>本研究实际上关注的是脑网络功能组织方式，工作记忆任务只是一个载体。他们将FPN分为两个独立子网络的思想和Dixon等人（2018）在PNAS的文章一致，同时进一步做出了一些扩展。</li>
</ul>
</li>
</ul>
<h3 id="02-08"><a href="#02-08" class="headerlink" title="02/08"></a>02/08</h3><ul>
<li>supervised learning is about learning the relationship of information from data and labels.</li>
<li><del>Fire together</del> <strong>correlates (information) together, wire together</strong>.</li>
</ul>
<h3 id="03-08"><a href="#03-08" class="headerlink" title="03/08"></a>03/08</h3><ul>
<li>无脊椎动物有个共同特点，神经元分布在身体各处，没有高度集中（有些昆虫去掉了头，身体也能活动一段时间）。一般情况下，不集中的神经元可以帮助无脊椎动物建立条件反射，更快躲避危险。比如苍蝇，脚上的纤毛一旦感受到气流变化，腿就会自己起跳，接着中枢神经才会反应过来开始飞行。这对高等生物是有害的，身体无法统一调配，就很难发展处复杂的大脑，但章鱼却反其道而行之，演化出各行其道的神经调配方式，拥有九个大脑。</li>
</ul>
<h3 id="04-08"><a href="#04-08" class="headerlink" title="04/08"></a>04/08</h3><ul>
<li><p>multi-layer neural entworks</p>
<ul>
<li><p>Neuron types</p>
<blockquote>
<p>it’s not a conceptual revolution but it enable a lot of progress to just switch the type of neurons</p>
</blockquote>
<ul>
<li>binary Neurons (simply)</li>
<li>sigmoid (have a bad property that it would saturate and things wouldn’t converge properly)</li>
<li>ReLU</li>
</ul>
</li>
<li><p>backpropagate is just a practical application of chain rule, nothing really complicate in terms of the math, <strong>but it really surprising that it took so long for people to realize this was a good idea</strong>.</p>
</li>
</ul>
</li>
<li><p>we are prisoner of the concepts that are enabled by hardware. The hardware at our disposal really determines the ideas that we allow ourself to think of.</p>
<blockquote>
<p>why back propagation which is such a simple concept was not invented before, the reason is because that people have the wrong neurons (binary neurons).</p>
</blockquote>
</li>
</ul>
<h3 id="05-08"><a href="#05-08" class="headerlink" title="05/08"></a>05/08</h3><ul>
<li>函数概念是人类一个很伟大的发现，价值不下于对于数的发现，也是高度抽象的产物。不过函数的思想却很早，至少在公元前就有了：因果关系，也即有因必有果，一个因对应一个或多个果，或者一个果对应多个因。</li>
</ul>
<h3 id="06-08"><a href="#06-08" class="headerlink" title="06/08"></a>06/08</h3><ul>
<li><p>涌现现象无非是复杂系统中诸多现象中最神秘莫测的一个。所谓的<strong>涌现就是指复杂系统在宏观所展现出来的，无法归约到微观的特性或规律</strong>。</p>
<ul>
<li>因果涌现理论就是在说某些特殊系统的宏观层面是有可能自发产生出超越微观动力学的新的因果箭头的。</li>
</ul>
</li>
<li><p>Lynn, Christopher W., and Danielle S. Bassett. 2019. “The Physics of Brain Network Structure, Function and Control.” Nature Reviews Physics 1 (5): 318–32. <a href="https://doi.org/10.1038/s42254-019-0040-8" target="_blank" rel="noopener">link</a>.</p>
<ul>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=MzIzMjQyNzQ5MA==&mid=2247530155&idx=1&sn=63d065095bd06eb95ccade24ada674b8&chksm=e8972c26dfe0a5305dc1fd32e2524d9c9e7479776b8a200ac506fdccfbe58b3e9ae4482b6196&scene=21#wechat_redirect" target="_blank" rel="noopener">前沿综述：大脑结构网络、功能网络和网络控制中的物理学</a></p>
</li>
<li><p>在网络的视角中，大脑中的<del>基本物理单元（神经元）</del> 被看作节点，<del>单元之间的连接（轴突）</del> 被看作连边。</p>
<ul>
<li>信息处理单元被看作节点，</li>
</ul>
</li>
<li><p><strong>高聚合度和短平均距离之间的权衡</strong>，体现了网络的小世界和具有中心节点的特性。这可能与促进脑内信息的拆分和整合有关，从而减少处理外来信息的最小计算步骤[4-6]。</p>
</li>
<li><p>脑物理网络定义的连通性是指物理上可测量的神经连接（如神经元之间的突触和脑区之间的白质束），然而功能网络定义的连通性是基于两个单元之间动态过程是否相似[18](<a href="https://doi.org/10.1038/nrn2575" target="_blank" rel="noopener">paper link</a> Cited by 9441)。</p>
</li>
<li><p>但想要将结构与功能联系起来并不容易，他们之间多呈现非线性关系，这也是学界聚焦的一个重要课题[19, 20].</p>
<ul>
<li>Deco, Gustavo, Giulio Tononi, Melanie Boly, and Morten L. Kringelbach. 2015. “Rethinking Segregation and Integration: Contributions of Whole-Brain Modelling.” Nature Reviews Neuroscience 16 (7): 430–39. <a href="https://doi.org/10.1038/nrn3963" target="_blank" rel="noopener">link</a>.</li>
<li></li>
</ul>
</li>
<li><p>Human brain functional networks are embedded in anatomical space and have <strong>topological properties</strong>—<u>small-worldness, modularity, fat-tailed degree distributions</u>—that are comparable to many other complex networks.</p>
<blockquote>
<p>Although a sophisticated set of measures is available to describe the topology of brain networks, the selection pressures that drive their formation remain largely unknown. Here we consider generative models for the probability of a functional connection (an edge) between two cortical regions (nodes) separated by some Euclidean distance in anatomical space. In particular, we propose a model in which <strong>the embedded topology of brain networks emerges from two competing factors</strong>: a distance penalty based on the cost of maintaining long-range connections; and a topological term that favors links between regions sharing similar input.</p>
</blockquote>
</li>
<li><p>顺用统计力学里的洞察，对于脑网络，人们也是遵循先对<strong>单个神经元动态过程进行模拟</strong>[21-23]，然后到不同脑区的<strong>平均场神经质量模型</strong>的构建[24, 25]，最后尝试实现全脑的网络模拟[15, 26]。文中将神经动态过程模型分成两类：<strong>人工模型（Artificial Models）</strong> 和 <strong>生物物理模型（Biophysical Models）</strong> .</p>
</li>
<li><p>类伊辛模型也被用来解释神经元系宗中的临界或雪崩状行为[28] (<a href="https://link.aps.org/doi/10.1103/PhysRevE.92.052804" target="_blank" rel="noopener">link</a>).</p>
</li>
<li><p>为了得以解释从单个神经元中涌现出的大尺度神经行为，研究者们开始在Wilson-Cowan的群体动力学模型基础上[25]，发展叫做神经质量模型（Neural Mass Models）的，大尺度神经活动的平均场描述。</p>
</li>
<li><p>在控制理论（Control Theory）中，追求的目标是如何推动一个系统到理想的状态中去[44, 45]。</p>
</li>
</ul>
</li>
</ul>
<h3 id="07-08"><a href="#07-08" class="headerlink" title="07/08"></a>07/08</h3><ul>
<li><p><strong><em><a href="https://mp.weixin.qq.com/s/rGkSbDTzGl0WWHhsgpymSA" target="_blank" rel="noopener">在心理学、认知科学、神经科学中，如何构建formal理论？一份实用入门操作指南</a></em></strong></p>
<ul>
<li><p>在心理学、认知科学、神经科学的学术论文中，我们常常会看到，也常常会写下这样的句子：某某理论支持我们的发现，我们的结果支持某某理论。但是当我们写下这样的句子时，我们真的知道句子中每一部分代表的含义吗？理论是什么？其核心假设是什么？我们的发现是现象吗？还是仅仅是数据？什么是现象？什么是数据？当我们说“支持”的时候，“支持”到底是什么意思？在我们对以上问题都不明确的情况下，为什么可以得出我们的结果支持该理论这样的结论呢？<br><a href="07082021_formal_theory.png"></a></p>
</li>
<li><p>在讲如何构建理论之前，我们先来看三个核心的概念：数据、现象和理论.</p>
<ul>
<li>数据：可以定义为直接的观察或者报告.</li>
<li>现象：是关于这个世界的稳定的一般性的特征或者模式，它们等待科学家去解释.</li>
<li>理论：是一系列有组织的命题，这些命题可以表达为一般性的准则.</li>
</ul>
</li>
<li><p>数据、现象和理论之间的关系</p>
<ul>
<li>当数据可以在不同的情境和实验条件下被反复发现时，其就变成了现象。</li>
<li>而一个稳定的现象可以用来预测未来实验中可能观测到的数据结果。</li>
<li>现象需要抽象出其中的核心要素，用计算和数学模型构建出其中的关系，这就产生了理论。而理论可以用来加深我们对现象背后规律的理解。<br><a href="07082021_relationship_data_phe_theory.png">relationship_data_phe_theory</a></li>
</ul>
</li>
<li><p>当缺少明确的理论时，我们提出的很多实验假设往往是研究者拍脑袋想出来的，并没有严格的逻辑推导和理论依据。</p>
</li>
<li><p>如何来构建理论呢？Borsboom等人提出一个可以实操的操作指南 (Borsboom et al., 2021)，其希望可以指导更多的心理学研究者进行formal理论构建的研究。</p>
<blockquote>
<p>理论构建分成5个步骤，我们来逐一展开。第一步，发现现象。第二步，发展理论原型。第三步，建立formal理论。第四步，通过数学分析，检验理论是否可以解释现象。第五步，对理论进行全面的评价，比如是否可预测等。</p>
</blockquote>
</li>
</ul>
</li>
<li><p>自从在普林斯顿大学毕业后，温伯格就一直着迷于研究在对称性原理下，为物理理论提供演绎基础的可能性，尤其是考虑可重整性。</p>
</li>
<li><p>可重整性是将量子场论与现实联系起来的技术，它提供了一种科学合理的方法来处理计算中出现的无穷大。</p>
</li>
<li><p>在学习过程中，把突触可塑性和网络可塑性统一在一起，看他们之间是否有耦合性。</p>
</li>
<li><p>大脑网络的改变，主要通过抑制性神经细胞来控制。控制抑制性神经元的兴奋性输入，可以控制抑制性神经元的放电，从而控制大脑网络的改变。</p>
</li>
<li><p>有一个很好的很明显的研究兴奋性输入的功能，LTP (long term potential)</p>
</li>
<li><p>近年来，许多著名的学者都提出了对人工智能未来发展趋势的构想。例如，有「深度学习」三驾马车之称的Hinton、Bengio 和 LeCun 提出，<strong>人工智能将会结合表征学习和复杂的推理(representation learning with complex reasoning)</strong>。</p>
</li>
<li><p>知识驱动和数据驱动相结合的表征和推理是机器智能研究的重要基础。(<strong>knowledge driven and data driven representation</strong>)</p>
</li>
<li><p>医学界有一句话：偶尔治愈，常常帮助，总是安慰（Cure sometimes, relief often, comfort always）。</p>
</li>
<li><p>不可否认，我们的很多想法、理论和模型几乎都是错的，大脑远比我们的粗略估计复杂。认为自己是对的，是一种很天真的想法。我们最多只能希望，我们的模型和想法错得很有意思，这个想法是值得思考的。</p>
</li>
</ul>
<ul>
<li><p>先秦诸子，百家争鸣-易中天。</p>
<ul>
<li>墨家-平等、互利、博爱。</li>
<li>道家-真实、自由、宽容。</li>
<li>法家-公平、公开、公正。</li>
<li>儒家-仁爱、正义、自强。</li>
</ul>
</li>
<li><p>还原论</p>
<ul>
<li><p>从生物性的神经元到MCP模型的层层简化，正体现了人类认知事物所秉承的还原论的思路，这其中的还原过程分为三步。</p>
<ul>
<li><strong>去除生物性是还原的第一步</strong>。其中的关键在于保留功能，去除物理载体，如突触、离子通道、膜等，也即从生物性的神经元变成模型。</li>
<li><strong>去除非线性是还原的第二步</strong>。世界上绝大部分非线性的事物都可以用线性表征，用线性近似非线性的目的在于简化运算，提升可实现性。</li>
<li><strong>二值化是还原的第三步</strong>。将线性的模型转化为0和1这两种状态，<u>通过逻辑学来描述思维</u>。这时神经元的运作不再是一个简单的生物活动，而是在进行一种思维方式的工作，其目的是为了实现人工智能。</li>
</ul>
</li>
<li><p>简单说来，科学的逻辑只有一个，那便是用还原来寻找规律性的简单；从科学到产业的路径也只有一条，那便是用构造来重建可控制的复杂。</p>
</li>
</ul>
</li>
<li><p>1957年，心理学家罗森布拉特（Rosenblatt）创造了第一个人工神经网络，叫做感知机（Perceptron）。他将<strong>感知机定义为一个感觉、识别、记忆和反应</strong>，像人类思维一样的机器。但是感知机有一个先天的缺陷，就是它不能解决疑惑的问题，只能机械地做出反应。因此，科学界关于神经网络的研究很快便沉寂下去。</p>
</li>
</ul>
<h2 id="week-August-2"><a href="#week-August-2" class="headerlink" title="week August 2"></a>week August 2</h2><h3 id="09-08"><a href="#09-08" class="headerlink" title="09/08"></a>09/08</h3><ul>
<li><p>脑科学的未来：对7位神经科学家的访谈</p>
<ul>
<li>您对年轻的神经科学家有什么建议吗？<br>-波佩尔：如今你需要拥抱技术并努力积累技术技能，但我的建议是仔细阅读旧文献，阅读这个领域内的奠基性文献。例如，我要求实验室的每个人都要读<strong>大卫·马尔（David Marr）的《视觉计算理论》（Vision）的第一部分</strong>。再就是阅读<strong>哲学文本</strong>，比如<strong>杰瑞·福多（Jerry Fodor）的The Modularity of Mind（《心灵模块性》）</strong>。</li>
<li>马苏德·侯赛因（Masud Husain）<ul>
<li>请介绍一下您的研究方向。我在健康的人们身上研究神经科学，尝试了解大脑如何把<strong>注意力聚焦在信息上并记住它们</strong>，人们怎样被激励，做事的动力又源自何处。我也<strong>想知道注意力、记忆和动机过程在神经疾病患者身上发生的变化</strong>。我很幸运既可以做基本的神经科学研究，又可以让这些成果有临床应用。</li>
<li><strong>注意力和工作记忆是大脑信息的把关者</strong>，是它们在控制有多少信息被选择，有多少信息需要被长期地记住。 (工作记忆是怎么成为把关者的？？？)</li>
<li>您认为神经科学界面临的最大问题和挑战是什么？我认为最紧迫的挑战可能是<strong>缺乏好的理论框架来理解认知功能</strong>。我们在生成大量实验数据方面没有问题，但缺乏好的理论，这些理论对这个领域的进展可能是很重要的。</li>
</ul>
</li>
<li>兰迪·麦金托什（Randy McIntosh）<ul>
<li>我们利用实验数据（成像数据、生理数据）建立了一个计算机模拟平台。这些数据被用来建立基于连接组的模型，可以帮我们理解正常工作的大脑，也可以用来检视患者群体大脑的动态变化。了解如何调整模型中的参数来改善动态，或许能启发一些治疗干预的手段。</li>
<li>将大脑模拟为神经网络为什么重要？这是由几件不同的事推动的。首先是脑成像研究的激增，其中最重要的技术突破之一就是我们如今能观察到大脑的连接。<strong>大体上，我觉得人们渐渐意识到为了理解大脑的功能，就应该理解这些网络以及它们是如何支持神经功能的</strong>。而问题在于，当我们开始研究大脑网络时，并没有已有的语言可以用来表达这种结构。因此我们引入了网络科学的概念，其能将网络描述得更直观，让我们能够用一种全新的语言描述大脑。</li>
<li>您认为网络神经科学中有哪些亟待解决的难题和挑战？最难的部分是将网络的特性与认知或心理功能联系起来。我之所以这么说，是因为<strong>隐含在大脑的神经网络特征中的另一属性，即所谓的复杂系统或复杂适应性系统，并没有得到广泛讨论</strong>。它们有十分特别的性质：第一，它们是动态的，随时间变化而改变；第二，它们发生在不同尺度上，这意味着不同尺度的动态行为的整合才是系统正常运行的关键。因而，并没有一个所谓的测量大脑活动的最佳尺度，当我们拘泥于一个单一的尺度时，往往错失了很多其他东西。</li>
</ul>
</li>
<li>徐敏 中国科学院神经科学研究所研究员  神经环路与行为调控研究组组长<ul>
<li>您认为脑科学有哪些亟待解决的难题和挑战？脑科学目前还处于起步阶段，包括在神经系统的发育、各种行为的神经基础、神经系统的疾病等问题上都没有一个明确的答案。</li>
<li>您如何看待如今研究中的方法论，尤其是您的研究领域？迄今为止，绝大多数睡眠或其他行为的神经基础研究都基于这样的思路：假定存在某个脑区或某些神经环路对于一个特定的行为是必须的。换句话说，我们似乎总是在下意识地寻找控制特定行为的中心。以睡眠为例，一方面我们发现很多脑区对睡眠都有调控作用，但还未发现抑制哪个脑区后睡眠行为就消失了；另一方面，我们借助大规模神经记录技术发现，即使是很简单的行为也会引起全脑尺度上神经活动的改变。这些都表明，<strong>寻找负责特定行为的中心脑区的思路可能存在局限性</strong>。</li>
</ul>
</li>
</ul>
</li>
<li><p>Distance metric learning is a branch of machine learning that aims to learn distances from the data, which enhances the performance of <strong>similarity-based algorithms</strong>.</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/80656461" target="_blank" rel="noopener">Metric Learning科普文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/108920087" target="_blank" rel="noopener">如何搞懂metric learning</a><ul>
<li>Metric Learning问题定义:x表示样本，y表示label。metric learning的目标就是学习一个变换函数（线性非线性均可）L把数据点从原始的向量空间映射到一个新的向量空间，在新的向量空间里相似点的距离更近，非相似点的距离更远，度量更符合任务要求，从而让KNN的表现更好。而deep metric learning，就是用深度神经网络来拟合这个变换函数。<strong>度量学习，其实是利用数据在空间中的相对和绝对位置，来帮助我们实现对样本空间的特征学习</strong>。</li>
</ul>
</li>
</ul>
</li>
<li><p>trajectory similarity measures <a href="https://pattern.swarma.org/paper?id=eea13aea-e879-11eb-a5fb-0242ac170008" target="_blank" rel="noopener">Computing Trajectory Similarity in Linear Time: A Generic Seed-Guided Neural Metric Learning Approach</a></p>
<ul>
<li><a href="https://pattern.swarma.org/path?id=133" target="_blank" rel="noopener">时序数据表示学习路径</a></li>
</ul>
</li>
<li><p><a href="https://pattern.swarma.org/path?id=132" target="_blank" rel="noopener">网络科学学习路径</a></p>
</li>
<li><p>Mean-field games (MFGs) is an emerging field that models large populations of agents.</p>
</li>
<li><p>Any fool can know. The point is to understand.</p>
</li>
<li><p>毛发动文革有两方面的因素，一个是集中体现了他对他所理想的社会主义的追求，第二个，是毛认为他在文革前期已经大权旁落，急于追回。这两方面的因素，在文革的全部过程中间，特别是初期，是紧密的缠绕在一起的。</p>
</li>
</ul>
<h3 id="11-08"><a href="#11-08" class="headerlink" title="11/08"></a>11/08</h3><ul>
<li><p>Internal excitation within</p>
</li>
<li><p>Theories on the functions of the hippocampal system are based largely on two fundamental discoveries: the amnestic consequences of removing the hippocampus and associated structures in the famous patient H.M. and the observation that spiking activity of hippocampal neurons is associated with the spatial position of the rat.</p>
</li>
<li><p>Notably, hexagonal symmetry is not a fundamental requirement for metric representation of the environment. For instance, a pattern with 90 degrees symmetry could equally well form the basis for a metric system (a grid).</p>
</li>
<li><p>我这几年一直强调理解哲学跟理解自然科学是不一样的。自然科学本质上是经验科学，经验科学的发展意味着一种经验的、实证的积累，所以经验科学不断在积累。但是哲学不一样，哲学一开始就是对世界人生最根本问题的整体的思考，不是局部的思考。</p>
</li>
<li><p>哲学的不发展意味着哲学的根本洞见的不发展，也就是说孔子、老子、孟子、庄子他们看到的最根本的那些哲学的见解、那些哲学的洞见是无法超越和发展的，这是哲学的不发展，所以只是回归。</p>
</li>
<li><p>战国时期几乎所有的哲学家都把“百家争鸣”当作一个根本问题要去解决，而不是像我们今天把它作为一个值得欣喜的局面来接受的。</p>
</li>
<li><p>“四辞”就是四种错误思想言论的类型。孟子说天下的错误言论其实无非就是四种错误思想言论的类型——诐、淫、邪、遁之辞。</p>
<blockquote>
<p>但是，这四种错误思想言论不是没有关系的。表面上看是四种错误思想言论的类型，其实是错误思想言论的四个发展阶段，根本上讲错误的根源是统一的。</p>
</blockquote>
<ul>
<li>“<strong>诐辞知其所蔽</strong>”。“诐辞”这个“诐”字就是不平，不平之辞、不正之辞、不公之辞。其实诐辞根本上在于<strong>立场的偏颇</strong>，只考虑一边，不考虑另一边；看问题想道理，只考虑一边不考虑另一边，就意味着不平、不正，就意味着偏颇。凡事皆有两端，只考虑一端不考虑另一端，这就意味着普遍性的缺乏。</li>
<li>“淫辞知其所陷”。淫辞就是诐辞的进一步发展，“淫”就是过度的意思。</li>
<li>“邪辞知其所离”。再往前发展，就到邪辞的阶段了。邪辞跟淫辞的区别是，淫辞虽然是错的，但是它还要打道义的旗号，到邪辞的时候，连道义的旗号都不用打。</li>
<li>中国哲学孔孟老庄，其实都没有走经验认知的道路，都没有试图通过经验的道路达到对世界根本原理的认识，都走的是反观内省的道路，向内寻求的道路。</li>
<li>在孟子看来，“思”才是认知那个普遍原理的真正正确的道路，甚至可以说是唯一的道路。<h3 id="13-08"><a href="#13-08" class="headerlink" title="13/08"></a>13/08</h3></li>
</ul>
</li>
<li><p>甘利俊一先生在智源大会上的报告 <a href="https://mp.weixin.qq.com/s/yHw9l4d1Pr6K3SUPIbC2jQ" target="_blank" rel="noopener">吴思解读</a></p>
<ul>
<li>在这些工作中，大家能感受到甘利先生善于抓住问题本质、并用简单例子来阐明核心思想的、理论研究的最高境界。</li>
<li>报告题目为：<strong>深度学习的统计神经动力学</strong>，主要内容涉及<strong>用信息几何方法对深度随机权值网络的研究</strong>。深度学习技术近年来在计算机视觉、语音识别等任务上取得了巨大成功，但是其背后的数学理论发展却很滞后。甘利先生<strong>指出统计神经动力学可能为我们理解深度学习提供重要的理论工具</strong>。</li>
<li>对随机连接权值网络的统计动力学研究主要是<strong>揭示网络的宏观统计动力学性质</strong>。近年来，<u>Poole等人（2016），S.Schoenholz等人（2017）</u>开始将统计神经动力学方法应用于深度神经网络的研究。</li>
<li>互馈随机网络可以产生丰富的神经动力学现象。甘利先生基于统计神经动力学方法，在1968年发现了<strong>互馈随机网络可以产生神经振荡现象</strong>[4]（类似的研究在1971年才被Wilson-Cowan提出[5]）。在1972年，甘利先生提出了联想式记忆模型[6]（类似的工作由Hopfield 1982年提出[7]）。互馈随机网络还可以产生混沌动力学现象（Sompolinsky最早研究[8]）。</li>
<li>Pool等在2015年采用统计神经动力学方法研究了深度随机神经网络，展示了输入信号是如何在深度随机网络中一层层被转换并传播，其发现随着网络层的加深，深度随机网络的表征会产生有序到混沌的相变过程。Pool等还进一步证实，这种深度随机网络表征的映射函数并不能有效地被浅层网络表征。</li>
<li>Schoenholz等在2017年将统计神经动力学方法用于研究误差的反向传播过程，证实了当优化处于混沌状态的深度随机网络时，反向传播算法的局限性。</li>
</ul>
</li>
<li><p>深度学习最擅长高维数据建模，但是泛化性和可解释性差。传统的符号模型可以很好地解释和泛化，但高维数据又不好处理。</p>
</li>
</ul>
<ul>
<li>What reductionism allows you to do is to take a complex problem and focus on one component of it and try to understand it in some detail. And sometimes you can just do it by focusing on one component, other times it requires selecting a particular biological system if you are working in biology, in which that component is prominent or easy to study. And that allows you to study in depth the problem. It will be hard to do if you looked at it in all its complexities.</li>
</ul>
<ul>
<li><a href="https://www.youtube.com/watch?v=Q-B_ONJIEcE" target="_blank" rel="noopener">Steven Pinker: Linguistics as a Window to Understanding the Brain</a><ul>
<li>Language is an intricate talent and it’s not surprising that the science of language should be a complex discipline.<ul>
<li>How language works. It includes the study of how language itself works, including:<ul>
<li>grammar, the assembly of words, phrases and sentences</li>
<li>phonology, the study of sound</li>
<li>semantics, the study of meaning</li>
<li>pragmatics, the study of the use of language in conversation.</li>
</ul>
</li>
<li>How it is processed. Scientists interested in language also study how it is processed in real time, a field called psycholinguistic</li>
<li>How it is acquired. How it is acquired by children, the study of language acquisition.</li>
<li>How it computed. how it is computed in brain, the discipline called neurolinguistics.</li>
</ul>
</li>
<li>Language is not thought, but a way of expressing thought.</li>
<li>Sapir-Whorf Hypothesis: language systematically influences how one perceives and conceptualizes the world.</li>
</ul>
</li>
</ul>
<h3 id="14-08"><a href="#14-08" class="headerlink" title="14/08"></a>14/08</h3><ul>
<li>H.M.’s major Symptom: Cannot convert new short-term memory to new long-term memory.</li>
<li>Long-term memory requires gene expression and growth of new synaptic connections.</li>
<li>There are two forms of long-term memory:<ul>
<li>Implicit. Motor and perceptual skills. Amygdala, Cerebellum, Reflex pathways</li>
<li>Explicit. Facts and events. Hippocampus, Medial temporal lobe.</li>
</ul>
</li>
</ul>
<h3 id="17-08"><a href="#17-08" class="headerlink" title="17/08"></a>17/08</h3><ul>
<li>图神经网络更擅长知识发现，符号表达泛化能力更强[[keywords#^178af8]]</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/diary/" rel="tag"># diary</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
                <a href="/2019/11/11/Topics/" rel="next" title="Topics">
                  <i class="fa fa-chevron-left"></i> Topics
                </a>
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
                <a href="/2019/11/11/Papers/" rel="prev" title="Papers">
                  Papers <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    
  <div class="comments" id="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#2021"><span class="nav-number">1.</span> <span class="nav-text">2021</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#week-July-3"><span class="nav-number">1.1.</span> <span class="nav-text">week July 3</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#22-07"><span class="nav-number">1.1.1.</span> <span class="nav-text">22/07</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#23-07"><span class="nav-number">1.1.2.</span> <span class="nav-text">23/07</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#25-07"><span class="nav-number">1.1.3.</span> <span class="nav-text">25/07</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#week-July-4"><span class="nav-number">1.2.</span> <span class="nav-text">week July 4</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#26-07"><span class="nav-number">1.2.1.</span> <span class="nav-text">26/07</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#27-07"><span class="nav-number">1.2.2.</span> <span class="nav-text">27/07</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#28-07"><span class="nav-number">1.2.3.</span> <span class="nav-text">28/07</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#29-07"><span class="nav-number">1.2.4.</span> <span class="nav-text">29/07</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#30-07"><span class="nav-number">1.2.5.</span> <span class="nav-text">30/07</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#31-07"><span class="nav-number">1.2.6.</span> <span class="nav-text">31/07</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#week-August-1"><span class="nav-number">1.3.</span> <span class="nav-text">week August 1</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#01-08"><span class="nav-number">1.3.1.</span> <span class="nav-text">01/08</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#02-08"><span class="nav-number">1.3.2.</span> <span class="nav-text">02/08</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#03-08"><span class="nav-number">1.3.3.</span> <span class="nav-text">03/08</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#04-08"><span class="nav-number">1.3.4.</span> <span class="nav-text">04/08</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#05-08"><span class="nav-number">1.3.5.</span> <span class="nav-text">05/08</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#06-08"><span class="nav-number">1.3.6.</span> <span class="nav-text">06/08</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#07-08"><span class="nav-number">1.3.7.</span> <span class="nav-text">07/08</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#week-August-2"><span class="nav-number">1.4.</span> <span class="nav-text">week August 2</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#09-08"><span class="nav-number">1.4.1.</span> <span class="nav-text">09/08</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-08"><span class="nav-number">1.4.2.</span> <span class="nav-text">11/08</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#13-08"><span class="nav-number">1.4.3.</span> <span class="nav-text">13/08</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-08"><span class="nav-number">1.4.4.</span> <span class="nav-text">14/08</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#17-08"><span class="nav-number">1.4.5.</span> <span class="nav-text">17/08</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Yifan Luo"
      src="/images/smile.jpg">
  <p class="site-author-name" itemprop="name">Yifan Luo</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">15</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>


      
      <script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
      <script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
      <div class="widget-wrap">
          <h3 class="widget-title">Tag Cloud</h3>
          <div id="myCanvasContainer" class="widget tagcloud">
              <canvas width="250" height="250" id="resCanvas" style="width=100%">
                  <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/TDL/" rel="tag">TDL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/books/" rel="tag">books</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/course/" rel="tag">course</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/diary/" rel="tag">diary</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ideas/" rel="tag">ideas</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/keywords/" rel="tag">keywords</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/limbo/" rel="tag">limbo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/meetings/" rel="tag">meetings</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/notes/" rel="tag">notes</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/papers/" rel="tag">papers</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/people/" rel="tag">people</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/project/" rel="tag">project</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/reading/" rel="tag">reading</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/topics/" rel="tag">topics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/websites/" rel="tag">websites</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/writing/" rel="tag">writing</a><span class="tag-list-count">1</span></li></ul>
              </canvas>
          </div>
      </div>
      


    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yifan Luo</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.5.0
  </div>

        












        
      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  




  <script src="/js/local-search.js"></script>








<script>
if (document.querySelectorAll('div.pdf').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/pdfobject@2/pdfobject.min.js', () => {
    document.querySelectorAll('div.pdf').forEach(element => {
      PDFObject.embed(element.getAttribute('target'), element, {
        pdfOpenParams: {
          navpanes: 0,
          toolbar: 0,
          statusbar: 0,
          pagemode: 'thumbs',
          view: 'FitH'
        },
        PDFJS_URL: '/lib/pdf/web/viewer.html',
        height: element.getAttribute('height') || '500px'
      });
    });
  }, window.PDFObject);
}
</script>





  

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://Luoji.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  function loadComments() {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: {page: {
            url: "http://luoji-neuron.tk/2019/11/11/notes%20diary/",
            identifier: "2019/11/11/notes diary/",
            title: "Diary"
          }
        }
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://Luoji.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  }
    window.addEventListener('load', loadComments, false);
</script>

</body>
</html>
